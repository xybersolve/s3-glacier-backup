#!/usr/bin/env bash
# ================================================================
# -*- mode: bash -*-
# vi: set ft=sh
# ****************************************************************
#
# DESCRIPTION
#    Backup using Glacier via S3
#
# SYNTAX & EXAMPLES
#    See 'SYNTAX' (below)
#
# ----------------------------------------------------------------
# IMPLEMENTATION
#    version         script 0.0.5
#    author          Greg Milligan
#    copyright       Copyright (c) 2018 http://xybersolve.io
#    license         GNU General Public License
#
# ================================================================
#  DEBUG OPTION
#    set -n  # Uncomment to check your syntax, without execution.
#    set -x  # Uncomment to debug this shell script
#
# ---------------------------------------------------------------
#
# TODO:
# ****************************************************************


# ---------------------------------------
# CONFIGFURATION
# ---------------------------------------
# strict environment
set -o errexit  # exit on command error status
set -o nounset  # no unreadonlyd variables
set -o pipefail # failr on pipe failures
trap 'echo "Aborting due to errexit on line $LINENO. Exit code: ${?}" >&2' ERR

# ---------------------------------------
# GLOBAL VARIABLES
# ---------------------------------------
# booleans
declare -ir TRUE=1
declare -ir FALSE=0
# script info

declare -r PROGNAME="$(basename ${0})"
declare -r VERSION=0.0.1
declare -r SUBJECT=""
declare -r KEYS=""
declare -ri MIN_ARG_COUNT=1
declare -r SYNTAX=$(cat <<EOF

    Script: ${PROGNAME}
    Purpose: Backup directory sets to AWS Glacier via S3
    Description: Define bucket for directory set backups in conf file, set conf
      using --backup=conf-file-name-part

    Usage: ${PROGNAME} [options]

    Options:
      --help:  help and usage
      --version: show version info

      Actions:
        --setup-bucket=mybucket: Make bucket and set lifecycle
        --backup=<conf-name>: Backup sets of directories into assigned buckets
            conf-name: defaults to 'xybersolve'
        --list: List or bucket contents, optionally by prefix
        --view: View bucket object info, optionally by prefix
        --size: View size of objects in bucket, optionally by prefix
        --size-all: View size of all buckets (can be slow)
        --restore: Restore objects back into S3 from Glacier (bucket/prefix or all)
        --get<=/path/to/file>: Get a file from S3 copying to local file
        --delete: Delete bucket or object, optionally using prefix
        --schedule: Schedule the archive to be run as cronjob


      Variables & Flags:
        --verbose: Enable feedback
        --local=/dir/file: Local file or directory
        --brief: Boolean flag for short display
        --dryrun: Just show what will be done
        --verbose: Show various steps in process

      Examples:
        ${PROGNAME} --setup-bucket=mybucket
        ${PROGNAME} --list [--bucket=mybucket] [--prefix=myprefix]
        ${PROGNAME} --view [--bucket=mybucket] [--prefix=myprefix]
        ${PROGNAME} --size [--bucket=mybucket] [--prefix=myprefix]
        ${PROGNAME} --size-all
        ${PROGNAME} --restore [--bucket=mybucket] [--prefix=myprefix]
        ${PROGNAME} --get='/path/to/file.jpg' --local='file-copy.jpg'
        ${PROGNAME} --get='/path/to/dir' --local='/directory'
        ${PROGNAME} --delete --bucket=mybucket --prefix=myprefix
        ${PROGNAME} --backup=<conf_name> [--verbose] [--dryrun]
        ${PROGNAME} --backup ('xybersolve' default)
        ${PROGNAME} --backup=gmp
EOF
)
# files & directories
declare -r SCRIPT_DIR="$( dirname ${0} )"
declare -r COMMON_FILE="${SCRIPT_DIR}/s3gback.common.sh"
declare -r EXCLUDE_LIST_FILE="${SCRIPT_DIR}/s3gback-exclude-list.dat"

# actions
declare -i SETUP_BUCKET=${FALSE}
#declare -i MAKE_BUCKET=${FALSE}
#declare -i SET_LIFECYCLE=${FALSE}
declare -i BACKUP=${FALSE}
declare -i VIEW=${FALSE}
declare -i LIST=${FALSE}
declare -i SIZE=${FALSE}
declare -i SIZE_ALL=${FALSE}
declare -i DELETE=${FALSE}
declare -i RESTORE=${FALSE}
declare -i GET=${FALSE}
declare -i SCHEDULE=${FALSE}

# flags
declare -i DRYRUN=${FALSE}
declare -i BRIEF=${FALSE}
declare -i VERBOSE=${FALSE}
declare -i SMS=${FALSE}
declare -i USE_S3CMD=${TRUE}


# ---------------------------------------
# COMMON FUNCTIONS
# ---------------------------------------
usage() {
  echo "${SYNTAX}"
}

error() {
  printf "\n%s\n" "Error: ${1}"
}

die() {
  error "${1}"
  usage
  printf "\n\n"
  exit "${2:-1}"
}

show_version() {
  printf "\n\n%s  %s\n\n\n" "${PROGNAME}" "${VERSION}"
  exit 0
}

show_help() {
  printf "\n\n"
  usage
  printf "\n\n"
  exit 0
}

confirm() {
  local prompt="${1:-Are you sure?}"
  # call with a prompt string or use a default
  read -r -p "${prompt}? [y/N] " response
  case "${response}" in
    [yY][eE][sS]|[yY])
       return 0 ;;
    *) return 1 ;;
  esac
}

# ---------------------------------------
# MAIN ROUTINES
# ---------------------------------------
source "${COMMON_FILE}" \
  || die "Unable to open configuration file: ${COMMON_FILE}" 1

# ********************************
# Bucket creation routines
# ********************************

# create & set lifecycle
__setup_bucket() {
  local -r bucket="${1:-${BUCKET_NAME}}"
  __make_bucket "${bucket}"
  #__set_policy "${bucket}"
  __set_lifecycle "${bucket}"
}

__make_bucket() {
  local -r bucket="${1:-${BUCKET_NAME}}"
  aws s3 mb "s3://${bucket}"
}

__set_policy() {
  local -r bucket="${1:-${BUCKET_NAME}}"
  aws s3api put-bucket-policy \
    --bucket "${bucket}" \
    --policy "file://${POLICY_FILE}"
}

__set_lifecycle() {
  # set lifecycle of bucket
  local bucket="${1:-${BUCKET_NAME}}"
  echo "set bucket lifecycle: ${bucket}"
  aws s3api put-bucket-lifecycle \
    --bucket "${bucket}" \
    --lifecycle-configuration "file://${LIFECYCLE_FILE}"
}

# check whether bucket exists
__bucket_exists() {
  # check if a bucket exists
  local bucket="${1:-${BUCKET_NAME}}"

  aws s3api head-bucket --bucket "${bucket}" 2>/dev/null \
    || return 1

  return 0

}
__ensure_bucket() {
  # check if bucket exists and create if necessary
  local -r bucket="${1:-${BUCKET_NAME}}"
  (( VERBOSE )) && echo "checking: ${bucket}"

  if __bucket_exists "${bucket}" 2>&1; then
    (( VERBOSE )) && echo "${bucket} exists"
  else
    (( VERBOSE )) && echo "make bucket: ${bucket}"
    __setup_bucket "${bucket}"
    #__make_bucket "${bucket}"
    #__set_policy
    #__set_lifecycle
  fi
}

__ensure_buckets() {
  # pull unique buckets from sets and ensure each exists
  local bucket
  local -a buckets=($( printf "%s\n" "${BACKUP_BUCKET_SETS[@]}" | sort -u ))
  # check if unque bukets exists
  for bucket in "${buckets[@]}"; do
    __ensure_bucket "${bucket}"
  done
}

__delete() {
  local bucket

  [[ -z ${BUCKET_NAME} ]] && die "Bucket name is a required argument" 5

  if confirm "Are you sure you want to delete: ${BUCKET_NAME}/${PREFIX}"; then
    [[ -n ${PREFIX} ]] \
      && echo aws s3 rm s3://${BUCKET_NAME}/${PREFIX} \
      || echo aws s3 rm s3://${BUCKET_NAME} --recursive
  else
    echo "User aborted!"
  fi
}

# ********************************
# Backup routines
# ********************************
__configure_backup() {
  # uses the variables from backup conf to build
  # * topic info: e.g., TOPIC_ARN
  # * backup info:

  # load backup configuration
  CONF_FILE="${SCRIPT_DIR}/s3gback.${CONF_NAME}.conf.sh"
  source "${CONF_FILE}" \
    || die "Unable to open configuration file: ${CONF_FILE}" 1

}

# backup sets as defined in config file
__backup() {
  local dir
  local bucket
  local src
  local dst
  local verbose=''
  #local dryrun=$( (( DRYRUN )) && echo '--dryrun' || echo '' )
  local dryrun=$( (( DRYRUN )) && echo '--dry-run' || echo '' )
  local verbose=$( (( VERBOSE )) && echo '--verbose' || echo '' )
  # TODO: Make into flag (arg driven)
  #local show_errors_only='--show-errors-only'
  local show_errors_only=''
  # backup all sets as defined in config file
  # make sure buckets exist
  __ensure_buckets

  for dir in "${!BACKUP_BUCKET_SETS[@]}"; do
    for bucket in "${BACKUP_BUCKET_SETS[${dir}]}"; do
      # backup each local directory into assigned bucket/dir
      # * src: LOCAL_BASE_DIR/dir
      # * dst: bucket/dir
      #   -- LOCAL_BASE_DIR/dir -> bucket/dir
      src="${LOCAL_BASE_DIR}/${dir}"
      dst="${bucket}/${dir}"
      (( VERBOSE )) && echo "Backing up: ${src} -> ${dst}"

      if (( USE_S3CMD )); then

        (( VERBOSE )) && echo "Using: s3cmd"

        s3cmd sync \
          --exclude-from=${EXCLUDE_LIST_FILE} \
          --delete-removed  --delete-after \
          --skip-existing \
          --preserve \
          --human-readable-size \
          ${show_errors_only} ${dryrun} ${verbose} ${src} s3://${dst}/

      else

        (( VERBOSE )) && echo "Using: aws s3"

        aws s3 sync --delete \
          ${src} s3://${dst} \
          --include "*" \
          --exclude "*/.DS_Store" \
          --exclude "*/.git/*" \
          --exclude "*/venv/*" \
          --exclude "*/.terraform*" \
          --exclude "*/site-packages/*" \
          --exclude "*/bower_components/*" \
          --exclude "*/.Python/*" \
          --exclude "*/lib/python*" \
          --exclude "*/include/python*" \
          --exclude "*/lock.json" \
          --exclude "*/node_modules/*" "${show_errors_only}" "${dryrun}"

        fi
      done
  done
}

__restore() {
  # using s3cmd
  #s3cmd restore --recursive s3://${BUCKET_NAME}/${PREFIX}/
  if [[ -n ${PREFIX} ]]; then
    s3cmd restore "s3://${BUCKET_NAME}/${PREFIX}"
  else
    s3cmd restore "s3://${BUCKET_NAME}" --recursive
  fi
}

__monitor_restore() {
  aws s3api head-object \
    --bucket ${BUCKET_NAME} \
    --key ${PREFIX}
}


__get_file() {
  # using s3cmd
  if [[ -n ${PREFIX} ]]; then
    s3cmd get "s3://${BUCKET_NAME}/${PREFIX}" "${LOCAL}"
  else
    s3cmd get --recursive "s3://${BUCKET_NAME}" "${LOCAL}"
  fi
}

# setup scheduled backuop using crontab
__schedule() {
  local cron_dir="${SCRIPT_DIR}/crontab"
  local cron_file="${cron_dir}/tmpcron"
  mkdir -p "${cron_dir}"
  crontab -l > "${cron_file}"

  # this is the meat of it
  # locally - ${HOME}/bin (SCRIPT_DIR) is where my bash script always reside
  # -- adjust accordingly
  echo "0 14 * * * ${SCRIPT_DIR}/s3gback --backup=xybersolve" >> "${cron_file}"
  echo "0 15 * * * ${SCRIPT_DIR}/s3gback --backup=gmp" >> "${cron_file}"
  echo "0 14 * * * ${SCRIPT_DIR}/s3gback --backup=ancillary" >> "${cron_file}"

  #install new cron file
  crontab "${cron_file}"
  # cleanup
  rm "${cron_file}"
}

# ********************************
# Feedback routines
# ********************************

__list() {
  [[ -n ${PREFIX} ]] \
    && aws s3 ls s3://${BUCKET_NAME}/${PREFIX} --recursive \
    || aws s3 ls s3://${BUCKET_NAME} --recursive
}

__view() {
   [[ -n ${PREFIX} ]] \
    && aws s3 ls s3://${BUCKET_NAME}/${PREFIX}/ --recursive \
    || aws s3 ls s3://${BUCKET_NAME}/ --recursive
}

__size_all() {
  # get size of all buckets
  s3cmd du "s3://"
}

__size() {

  # use s3cmd
  # s3cmd du s3://${BUCKET_NAME}/${PREFIX}
  if [[ -n ${PREFIX} ]]; then
    s3cmd du "s3://${BUCKET_NAME}/${PREFIX}"
  else
    s3cmd du "s3://${BUCKET_NAME}"
  fi

}

__view() {

  if [[ -n ${PREFIX} ]]; then
    aws s3api list-objects \
      --bucket ${BUCKET_NAME} \
      --prefix ${PREFIX} \
      --output table
  else
    aws s3api list-objects \
      --bucket ${BUCKET_NAME} \
      --output table
  fi
}

# ********************************
# Messaging routines
# ********************************
__configure_topic() {
  TOPIC_ARN="arn:aws:sns:${REGION}:${ACCOUNT_ID}:${TOPIC_NAME}"
}

__create_topic() {
  [[ -z "${TOPIC_ARN}" ]] && die "TOPIC_NAME is not defined" 3

  aws sns create-topic --name "${TOPIC_NAME}"
}


__subscribe() {
  [[ -z "${TOPIC_ARN}" ]] && die "TOPIC_ARN is not defined" 4
  [[ -z "${ADMIN_EMAIL}" ]] && die "ADMIN_EMAIL is not defined" 5

  aws sns subscribe \
    --topic-arn "${TOPIC_ARN}" \
    --protocol email \
    --notification-endpoint "${ADMIN_EMAIL}"
}

__unsubscribe() {
  local url='https://sns.us-west-2.amazonaws.com/unsubscribe.html?SubscriptionArn=arn:aws:sns:us-west-2:734741078887:S3-Glacier-Backup:7b399f3f-8ac4-4fb7-82a3-4447274632e4&Endpoint=xybersolve@gmail.com'
  local url2="https://sns.${REGION}.amazonaws.com/unsubscribe.html?SubscriptionArn=arn:${TOPIC_ARN}:${SUBSCRIPTION_ID}&Endpoint=${ADMIN_EMAIL}"
}

__check_topic() {
  aws sns list-subscriptions --output text \
    | grep -q "${TOPIC_ARN}" \
      && return 0 \
      || return 1

}

__setup_topic() {
  if ! __check_topic; then
    (( VERBOSE )) && echo Creating topic: does not exist
    __create_topic
    __subscribe
  else
    echo topic exists
  fi
}


__send() {
  local message="${1:-${DEFAULT_MESSAGE}}"
  local datetime=$( date +%Y%m%d:%H:%M:%S )

  aws sns publish \
    --topic-arn "${TOPIC_ARN}" \
    --subject "${TOPIC_SUBJECT}" \
    --message "${message} at ${datetime}"
}

__get_subscriptions() {
  aws sns list-subscriptions
}

__delete_topic() {

  if __check_topic; then
    aws sns delete-topic \
      --topic-arn "${TOPIC_ARN}"
  else
    echo "Topic does not exist!"
  fi
}


__get_opts() {
  while (( $# > 0 )); do
    local arg="${1}"; shift;
    case ${arg} in
      # Common
      --help)    show_help                      ;;
      --version) show_version                   ;;

      # Actions
      --setup-bucket)
        SETUP_BUCKET=${TRUE}
        [[ ${arg} =~ '=' ]] && BUCKET_NAME="${arg#*=}"
        ;;
      --backup*)           BACKUP=${TRUE}
        [[ ${arg} =~ '=' ]] && CONF_NAME="${arg#*=}"
        ;;
      --list|--view)      LIST=${TRUE}          ;;
      --size)             SIZE=${TRUE}          ;;
      --size-all)         SIZE_ALL=${TRUE}      ;;
      --restore)          RESTORE=${TRUE}       ;;

      --delete)           DELETE=${TRUE}        ;;
      --schedule)         SCHEDULE=${TRUE}      ;;
      --get*)             GET=${TRUE}
        [[ ${arg} =~ '=' ]] && PREFIX="${arg#*=}"
        ;;
      --setup-topic) # debug
        __setup_topic
        exit 0
        ;;
      --delete-topic)
        __delete_topic
        exit 0
        ;;
      --send)
        __send
        exit 0
        ;;
      # Variables & Flags
      --brief)            BRIEF=${TRUE}         ;;
      --verbose)          VERBOSE=${TRUE}       ;;
      --dryrun)           DRYRUN=${TRUE}        ;;
      # --bucket*)
      #   [[ ${arg} =~ '=' ]] && BUCKET_NAME="${arg#*=}"
      #   ;;
      # --prefix*)
      #   [[ ${arg} =~ '=' ]] && PREFIX="${arg#*=}"
      #   ;;
      # --local*)
      #   [[ ${arg} =~ '=' ]] && LOCAL="${arg#*=}"
      #   ;;
      *) die "Unknown option: ${arg}" ;;
   esac
  done
  return 0
}

__dispatch() {
  # S3/Glaceir Backup
  (( SETUP_BUCKET )) && __setup_bucket
  #(( MAKE_BUCKET )) && __make_bucket
  #(( SET_LIFECYCLE )) && __set_lifecycle
  (( BACKUP )) && __backup
  (( LIST )) && __list
  (( VIEW )) && __view
  (( RESTORE )) && __restore
  (( GET )) && __get_file
  (( DELETE )) && __delete
  (( SIZE )) && __size
  (( SIZE_ALL )) && __size_all
  (( SCHEDULE )) && __schedule

  return 0
}

main() {
  (( ${#} < MIN_ARG_COUNT )) && die "Expects at least ${MIN_ARG_COUNT} arguments" 1
  (( $# > 0 )) && __get_opts "$@"

  __configure_backup
  __configure_topic
  __dispatch

  return 0
}
(( ${#} > 0 )) && main "${@}" || main

# *****************************************************
#
# RESOURCES FROM OTHER S3 SCRIPTS
#
# *****************************************************
# __restore_object() {
#   aws s3api restore-object \
#     --bucket "${BUCKET_NAME}" \
#     --key "${KEY}"
# }
#
#
# __list_objects() {
#   #-–prefix ${PREFIX_NAME} \
#   aws s3api list-objects \
#     -–bucket ${BUCKET_NAME}
#     #-–output json \
#     #-–query ‘Contents[?StorageClass==GLACIER].[Key]’
# }
#
# __restore_objects() {
#   for key in $(aws s3api list-objects-v2 --bucket ${BUCKET_NAME} --query "Contents[?StorageClass=='GLACIER'].[Key]" --output text); do
#     if [ $(aws s3api head-object --bucket ${BUCKET_NAME} --key ${KEY} --query "contains(Restore, 'ongoing-request=\"false\"')") == true ]; then
#       echo ${key}
#     fi
#   done
#
#   aws s3 ls s3://${BUCKET_NAME} | awk '{print $4}' \
#     | xargs -L 1 aws s3api restore-object \
#       --restore-request Days=5 \
#       --bucket ${BUCKET_NAME} \
#       --key
#   # awk 'BEGIN {FS="\t"}; {print $2}'
# }
#
# __restore3() {
#   # This will give you a nice list of all objects in the bucket with the bucket name stripped out
#   s3cmd ls -r s3://${BUCKET_NAME} \
#     | awk '{print $4}' \
#     | sed "s#s3://${BUCKET_NAME}/##" > ${SCRIPT_DIR}/glacier-restore.txt
#
#     for x in $( cat glacier-restore.txt ); do
#       echo "restoring $x"
#       aws s3api restore-object \
#         --restore-request Days=5 \
#         --bucket ${BUCKET_NAME} \
#         --key "${x}"
#     done
#
# }
#

# __check_restore() {
#   aws s3api list-objects \
#     -–bucket ${BUCKET_NAME} \
#     -–prefix ${PREFIX_NAME} \
#     -–output json –query \
#     ‘Contents[?StorageClass==GLACIER].[Key]’ \
#       | jq -r ‘.[] \
#       | “–key ‘\”” + .[0] + “‘\”” ‘ \
#       | xargs -L1 aws s3api head-object \
#       -–bucket ${BUCKET_NAME}
# }
#
# __upload() {
#   for file in "${FILES[@]}"; do
#     printf "Copying file: %s\n" "${file}"
#     aws s3 cp "${SITE_DIR}/${file}" s3://${SITE_BUCKET}
#   done
#
#   for dir in "${DIRS[@]}"; do
#     printf "Copying directory: %s\n" "${dir}"
#     aws s3 cp "${SITE_DIR}/${dir}/" s3://${SITE_BUCKET}/${dir} \
#       --recursive \
#       --exclude '*DS_Store*'
#   done
# }
#
# __delete_bucket() {
#   aws s3 rm s3://${BUCKET_NAME} --recursive --force
# }

# *****************************************************
#
# Below: Web site deloyment routines (resource)
#
# *****************************************************

# __sync() {
#   [[ -z "${DIRECTORY}" ]] \
#     && die "Directory argument required. --sync=<directory> is meant to sync a specified directory" 3
#
#   aws s3 sync "${SITE_DIR}/${DIRECTORY}" s3://${SITE_BUCKET}/${DIRECTORY}
# }
#
# __upload_dir() {
#   [[ -z "${DIRECTORY}" ]] && die "Directory argument is required" 2
#
#   aws s3 cp "${SITE_DIR}/${DIRECTORY}/" s3://${SITE_BUCKET}/${DIRECTORY} \
#     --recursive \
#     --exclude '*DS_Store*'
# }
#
# __delete_dir() {
#   aws s3 rm s3://${DIRECTORY}/ --recursive
# }
#
# __delete_all() {
#   aws s3 rm s3://${SITE_BUCKET}/ --recursive
# }

# __delete_logs() {
#   aws s3 rm s3://${LOG_BUCKET}/ --recursive
# }
#
# __remove_log() {
#   #__delete_logs
#   aws s3 rb s3://${LOG_BUCKET} --force
# }
#
# __list_site() {
#   aws s3 ls s3://${SITE_BUCKET}/
# }
#
# __size() {
#   aws s3api list-objects \
#     --bucket ${SITE_BUCKET} \
#     --output text \
#     --query "[sum(Contents[].Size), length(Contents[])]"
# }



  # using s3api
  # if [[ -n ${PREFIX} ]]; then
  #   aws s3api restore-object \
  #     --bucket ${BUCKET_NAME} \
  #     --key ${PREFIX} \
  #     --restore-request '{"Days":5,"GlacierJobParameters":{"Tier":"Standard"}}'
  # else
  #   aws s3api restore-object \
  #     --bucket ${BUCKET_NAME} \
  #     --restore-request '{"Days":5,"GlacierJobParameters":{"Tier":"Standard"}}'
  # fi

  # more loquaciopus methods of __restore3
  # if [[ -n ${PREFIX} ]]; then
  #   aws s3api list-objects \
  #     -–bucket ${BUCKET_NAME} \
  #     -–prefix ${PREFIX_NAME} \
  #     -–output json
  #     # -–query ‘Contents[?StorageClass==GLACIER].[Key]’ \
  #     #   | jq -r ‘.[] \
  #     #   | “–key ‘\”” + .[0] + “‘\”” ‘ \
  #     #   | xargs -L1 aws s3api restore-object \
  #     #     -–restore-request Days=7,GlacierJobParameters={Tier=Expedited} \
  #     #     -–bucket ${BUCKET_NAME}
  # else
  #   aws s3api list-objects \
  #     -–bucket ${BUCKET_NAME} \
  #     -–output json
  #     # -–query ‘Contents[?StorageClass==GLACIER].[Key]’ \
  #     #   | jq -r ‘.[] \
  #     #   | “–key ‘\”” + .[0] + “‘\”” ‘ \
  #     #   | xargs -L1 aws s3api restore-object \
  #     #     -–restore-request Days=7,GlacierJobParameters={Tier=Expedited} \
  #     #     -–bucket ${BUCKET_NAME}
  # fi


    # use s3api
    # if [[ -n ${PREFIX} ]]; then
    #   aws s3api list-objects \
    #     --bucket "${BUCKET_NAME}" \
    #     --prefix "${PREFIX}" \
    #     --output table \
    #     --query "[Contents[].Key, Contents[].Size]"
    # else
    #   aws s3api list-objects \
    #     --bucket "${BUCKET_NAME}" \
    #     --output table \
    #     --query "[Contents[].Key, Contents[].Size]"
    # fi
